{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMX5MN5z3aO+ZhK6R1oyQuA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GESNruqgyovE"},"outputs":[],"source":["# Movie review - Sentiment Analysis\n","\n","# Given a list of 10 reviews with labels\n","reviews = [\n","    'A very good film of Tony Stark',\n","    'Fantastic fighting scene , I love watching Jet Li',\n","    'Not bad ! I was impressed by the action scene',\n","    'The love story was amazing , but I do not prefer romantic movie at all',\n","    'The main actor was stupid',\n","    'I can not imagine such an ugggly guy like the main actor'\n","]\n","\n","labels = [\n","    'Positive',\n","    'Positive',\n","    'Positive',\n","    'Negative',\n","    'Negative',\n","    'Negative'\n","]\n"]},{"cell_type":"code","source":["# Rule: \"Good\", \"Fantastic\", \"Amazing\" - \"Bad\", \"Ugly\", \"Stupid\"\n","positive_words = [\"good\", \"fantastic\", \"amazing\"]\n","negative_words = [\"bad\", \"ugly\", \"stupid\"]\n","def review_classification(review):\n","  # lowercase all the words in the review\n","  review_lower = review.lower()\n","\n","  # split the text to separated words\n","  review_words = review_lower.split()\n","\n","  # Assign label based on the keywords\n","  for word in review_words:\n","    if word in positive_words:\n","      return 'Positive'\n","    if word in negative_words:\n","      return 'Negative'\n","\n","  # return neutral by default\n","  return 'Neutral'\n","\n","for review in reviews:\n","  print(review_classification(review))"],"metadata":{"id":"kxx7qhL0yugC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667315656088,"user_tz":-420,"elapsed":637,"user":{"displayName":"Sinh Vũ Trọng","userId":"06294132242307512072"}},"outputId":"d46b0baf-6849-462f-9f6d-a67d089a2c89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Positive\n","Positive\n","Negative\n","Positive\n","Negative\n","Neutral\n"]}]},{"cell_type":"code","source":["# Create vocabulary\n","\n","all_tokens = [] # we use all_tokens variable to store all the words in all the reviews\n","for review in reviews:\n","  review_lower = review.lower()\n","  tokens = review_lower.split()\n","  all_tokens = all_tokens + tokens\n","\n","print(all_tokens)\n","print(len(all_tokens))\n","vocab = set(all_tokens)\n","print(vocab)\n","print(len(vocab))"],"metadata":{"id":"7W4MW3gZKaM-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667316066692,"user_tz":-420,"elapsed":547,"user":{"displayName":"Sinh Vũ Trọng","userId":"06294132242307512072"}},"outputId":"f24ebd3e-9f53-49bc-f640-a442cf71a73a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['a', 'very', 'good', 'film', 'of', 'tony', 'stark', 'fantastic', 'fighting', 'scene', ',', 'i', 'love', 'watching', 'jet', 'li', 'not', 'bad', '!', 'i', 'was', 'impressed', 'by', 'the', 'action', 'scene', 'the', 'love', 'story', 'was', 'amazing', ',', 'but', 'i', 'do', 'not', 'prefer', 'romantic', 'movie', 'at', 'all', 'the', 'main', 'actor', 'was', 'stupid', 'i', 'can', 'not', 'imagine', 'such', 'an', 'ugggly', 'guy', 'like', 'the', 'main', 'actor']\n","58\n","{'prefer', 'was', 'good', 'fighting', 'love', 'guy', 'i', 'do', 'impressed', 'all', 'stupid', ',', 'main', 'scene', 'imagine', 'by', 'amazing', 'ugggly', 'actor', 'jet', 'romantic', 'very', 'a', 'bad', 'film', 'but', 'not', 'such', 'can', 'of', 'like', 'fantastic', 'li', 'movie', '!', 'stark', 'action', 'story', 'at', 'watching', 'the', 'an', 'tony'}\n","43\n"]}]},{"cell_type":"code","source":["# Vectorize a review\n","\n","def review_to_vector(review):\n","  review_vector = []\n","  review_lower = review.lower()\n","  review_tokens = review_lower.split()\n","  for token in vocab:\n","    if token in review_tokens:\n","      review_vector.append(1)\n","    else:\n","      review_vector.append(0)\n","  return review_vector"],"metadata":{"id":"5gsxyXaKKc1d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Vectorize a review\n","\n","reviews2vectors = [review_to_vector(r) for r in reviews]\n","print(reviews[0])\n","print(len(reviews2vectors))\n","print(reviews2vectors[0])"],"metadata":{"id":"YrBzJk3qKeY2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667316187844,"user_tz":-420,"elapsed":7,"user":{"displayName":"Sinh Vũ Trọng","userId":"06294132242307512072"}},"outputId":"616cc3a8-ae69-4c2f-c2dd-091c4e1baca5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A very good film of Tony Stark\n","6\n","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n"]}]},{"cell_type":"code","source":["# Use a Naive Bayes classifier in sklearn to build our model\n","\n","from sklearn.naive_bayes import MultinomialNB\n","\n","clf1 = MultinomialNB()\n","clf1.fit(reviews2vectors, labels)"],"metadata":{"id":"NV2TPJN6KjEH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667316222555,"user_tz":-420,"elapsed":1142,"user":{"displayName":"Sinh Vũ Trọng","userId":"06294132242307512072"}},"outputId":"d88ed83d-4327-4537-ad91-351eaf486933"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB()"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# A faster way, using sklearn pre-built library to convert reviews to vector\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer()\n","review_vectorizer = vectorizer.fit_transform(reviews)\n","print(vectorizer.get_feature_names())\n","print(review_vectorizer.toarray())"],"metadata":{"id":"LrZhHhb1KkCk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667316277740,"user_tz":-420,"elapsed":611,"user":{"displayName":"Sinh Vũ Trọng","userId":"06294132242307512072"}},"outputId":"e60da1d1-1fbe-4836-9f86-a255c9ceafa7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['action', 'actor', 'all', 'amazing', 'an', 'at', 'bad', 'but', 'by', 'can', 'do', 'fantastic', 'fighting', 'film', 'good', 'guy', 'imagine', 'impressed', 'jet', 'li', 'like', 'love', 'main', 'movie', 'not', 'of', 'prefer', 'romantic', 'scene', 'stark', 'story', 'stupid', 'such', 'the', 'tony', 'ugggly', 'very', 'was', 'watching']\n","[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0\n","  1 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n","  0 0 1]\n"," [1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0\n","  0 1 0]\n"," [0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0\n","  0 1 0]\n"," [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n","  0 1 0]\n"," [0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1\n","  0 0 0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"code","source":["# Use a Naive Bayes classifier in sklearn to build our model\n","\n","from sklearn.naive_bayes import MultinomialNB\n","\n","clf = MultinomialNB()\n","clf.fit(review_vectorizer, labels)"],"metadata":{"id":"oHnc-5GGKmP8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667316364632,"user_tz":-420,"elapsed":452,"user":{"displayName":"Sinh Vũ Trọng","userId":"06294132242307512072"}},"outputId":"1b727c09-6dc5-4081-cd4b-68fcb10f25b9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB()"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Try to predict the sentiment class of a new review\n","new_review = [\"Good actors . I love it\"]\n","new_review_vector = vectorizer.transform(new_review)\n","print('New review vector: ', new_review_vector.toarray())\n","print(clf.predict(new_review_vector))"],"metadata":{"id":"uZu3RnkGKni1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667316429670,"user_tz":-420,"elapsed":530,"user":{"displayName":"Sinh Vũ Trọng","userId":"06294132242307512072"}},"outputId":"0ca389d8-e981-4a53-924b-b535c8f4504e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["New review vector:  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0]]\n","['Positive']\n"]}]},{"cell_type":"markdown","source":["# Test with our own dataset"],"metadata":{"id":"IpkS2-rgKpLb"}},{"cell_type":"code","source":["# Read the dataset IMDB_dataset.csv, extract only 1000 rows\n","import pandas as pd\n","imdb_dataset = pd.read_csv('.../IMDB_Dataset.csv', nrows=1000)\n","# print out 5 first rows\n","print(imdb_dataset.head(5))"],"metadata":{"id":"7OHD3JB_Kts2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the list of reviews X\n","X = imdb_dataset['review'].values.tolist()\n","# Extract the labels y\n","y = imdb_dataset['sentiment'].values.tolist()"],"metadata":{"id":"8CtSfaCPK1vn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use CountVectorizer to convert each review in X to a vector of number\n","\n","# Show the list of unique words (vocabulary) in the dataset\n","\n","# Show the number of unique words in the vocabulary\n"],"metadata":{"id":"cvELjdTDK3ed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"ybolDNFPK7Kc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train a Naive Bayes model to classify review vectors\n","from sklearn.naive_bayes import MultinomialNB\n","\n","clf = MultinomialNB()\n","clf.fit(X_train, y_train)"],"metadata":{"id":"dpQGCc9bK9DM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict a new review\n","new_review = [\"'Doctor Strange' is not a perfect film. If you expect a movie to have depth and the point, you should skip, not only this one, but whole Marvel production.\"]\n","new_review_vectorized = vectorizer.transform(new_review)\n","print(clf.predict(new_review_vectorized.toarray()))\n","print(clf.predict_proba(new_review_vectorized.toarray()))"],"metadata":{"id":"z8wS_r50K-8U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate by the accuracy score\n","print(clf.score(X_test, y_test))"],"metadata":{"id":"1W6mN8T1LASl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Homework"],"metadata":{"id":"KizYacbmMkcG"}},{"cell_type":"code","source":["# Read the dataset IMDB_dataset.csv, extract only 2000 rows\n","import pandas as pd\n","imdb_dataset = pd.read_csv('.../IMDB_Dataset.csv', nrows=2000)\n","# print out 5 first rows\n","print(imdb_dataset.head(5))"],"metadata":{"id":"4dU1RM4tMooV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the list of reviews X\n","X = imdb_dataset['review'].values.tolist()\n","# Extract the labels y\n","y = imdb_dataset['sentiment'].values.tolist()"],"metadata":{"id":"Jd9IK-msMvKt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove stop words (a, an, the, another, is, are, was, were, will) from each comment in X\n","\n","# The whole list of stopwords can be found in stopwords.txt"],"metadata":{"id":"VS7awrc7My3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use TfidfVectorizer to convert each review in X to a vector of number\n","\n","# Show the list of unique words (vocabulary) in the dataset\n","\n","# Show the number of unique words in the vocabulary\n"],"metadata":{"id":"E92i4dLPM1JI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a Naive Bayes model to classify review vectors\n","from sklearn.naive_bayes import MultinomialNB\n","\n","clf = MultinomialNB()\n","clf.fit(X_train, y_train)\n","\n","# Evaluate by the accuracy score\n","print(clf.score(X_test, y_test))"],"metadata":{"id":"j9LQ2QqCNBr2"},"execution_count":null,"outputs":[]}]}